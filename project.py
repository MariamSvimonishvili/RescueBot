# -*- coding: utf-8 -*-
"""Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xILPBq65M7nOBY0NZUwS4o3SgupKns4C

# MNIST Hands-On
"""

import os
from PIL import Image
from torchvision import transforms

directory = "/content/data"
final_result = []
convert_to_tensor = transforms.Compose([
   transforms.Resize((256, 256)),
   transforms.ToTensor(),
   transforms.Normalize(mean = 0, std = 1),
])


for filename in os.listdir(directory):
    file_path = os.path.join(directory, filename)
    name, extension = os.path.splitext(filename)
    if extension != ".txt":
      if not os.path.exists(f"{directory}/{name}.txt"):
        continue

      image = Image.open(file_path) #passing the img path
      image_tensor = convert_to_tensor(image)
      with open(f"{directory}/{name}.txt", "r") as label:
        label = label.readlines()[0].split(" ")[0]  #identify 1st character

      final_result.append((image_tensor, int(label)))
#final_result

import torch


class MyIterableDataset(torch.utils.data.IterableDataset):
    def __init__(self, data_set):
        super(MyIterableDataset).__init__()
        self.dataset = data_set
    def __iter__(self):
        return iter(self.dataset)
    def __getitem__(self, k):
      for i, v in enumerate(self):
        if i == k: return v
    def __len__(self):
      i = 0
      for _ in self: i+= 1
      return i

"""#### Imports"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torchvision
import numpy as np
# %matplotlib inline
from tqdm import tqdm
from time import time
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from matplotlib import pyplot as plt
from torchvision import datasets, transforms

"""#### Helper Functions"""

""" Displays an image """
def display(image):
    print(f"Image Size: {image.shape}")
    plt.imshow(image)
    plt.show()

"""#### Global Constants"""

lr = 0.0001
epochs = 1000
batch_size = 41
home_dir = "/content"
dataset_path = f"{home_dir}"
cnn_path = f"{home_dir}/mnist_cnn.pth"
device = "cuda" if torch.cuda.is_available() else "cpu"

"""#### Preprocessing

#### Forward Propogation
"""

def forward(model, x):
  # result of first convolution
  output = model.conv_1(x)
  output = model.act_1(output)
  output = model.pool_1(output)

  # result of second convolution
  output = model.conv_2(output)
  output = model.act_2(output)
  output = model.pool_2(output)
  # output = model.softmax(output)

  # result of third convolution
  output = model.conv_3(output)
  output = model.act_3(output)
  output = model.pool_3(output)

  # result of firth convolution
  output = model.conv_4(output)
  output = model.act_4(output)
  output = model.pool_4(output)

  # result of fifth convolution
  output = model.conv_5(output)
  output = model.act_5(output)
  output = model.pool_5(output)

  output = model.conv_6(output)
  output = model.act_6(output)
  output = model.pool_6(output)

  # output = model.softmax(output)
  return model.out(output.view(output.size(0), -1))

"""#### Model"""

# class MNIST_CNN(torch.nn.Module):
#     def __init__(self):
#         super(MNIST_CNN, self).__init__()

#         ### First set of convolution ###
#         self.conv_1 = torch.nn.Conv2d(
#           stride = 1,          # we will convolve all of the image
#           padding = 1,         # how many pixels to add in each direction
#           in_channels = 3,     # we are sending in grayscale images with only 1 channel (1 image) - modify for colored ones
#           kernel_size = 3,     # the size of the first set of kernels
#           out_channels = 16,   # we output 16 filtered images
#           # number of output images each of
#           # size = (input_width − kernel_size + 2(padding_size)) / (stride) + 1
#           #      = (28 - 5 + 2(2)) / 1 + 1 = 28
#           # for our scenario image size: 256x256
#           # size = (256 - 5 + 2*2) / 1 + 1 =  256

#         )
#         self.act_1 = torch.nn.ReLU() # the activation for non-linearity
#         self.pool_1 = torch.nn.MaxPool2d(
#           stride = 2,
#           # we will convolve every other pixel splitting the size of the image by half
#           # size = 28 / 2 = 14
#           # size = 256 / 2 = 128
#           kernel_size = 2 # size of pooling kernel
#         )

#         ### Second set of convolution ###
#         self.conv_2 = torch.nn.Conv2d(
#           stride = 1,          # we will convolve all of the image
#           padding = 1,         # how many pixels to add in each direction
#           kernel_size = 3,     # the size of the second set of kernels
#           in_channels = 16,    # we are sending in grayscale images with 16 channels (16 images)
#           out_channels = 32,   # we output 32 filtered images
#           # number of output images each of
#           # size = (input_width − kernel_size + 2(padding_size)) / (stride) + 1
#           #      = (14 - 3 + 2(2)) / 1 + 1 = 14
#         )

#          # for 256x256 image
#          # size = (128 - 5 + 2 *2 ) / 1 + 1 = 128

#         self.act_2 = torch.nn.ReLU() # the activation for non-linearity
#         self.pool_2 = torch.nn.MaxPool2d(
#           stride = 2,
#           # we will convolve every other pixel splitting the size of the image by half
#           # size = 14 / 2 = 7
#           # size = 128/2 = 64
#           kernel_size = 2 # size of pooling kernel
#         )

#         ### third set of convolution ###
#         self.conv_3 = torch.nn.Conv2d(
#             stride = 1,          # we will convolve all of the image
#             padding = 1,         # how many pixels to add in each direction
#             kernel_size = 3,     # the size of the second set of kernels
#             in_channels = 32,    # we are sending in grayscale images with 16 channels (16 images)
#             out_channels = 64,   # we output 32 filtered images
#         )

#         self.act_3 = torch.nn.ReLU() # the activation for non-linearity
#         self.pool_3 = torch.nn.MaxPool2d(
#             stride = 2,
#             kernel_size = 2 # size of pooling kernel
#         )


#         ### Fully prediction connected layer ###
#         self.out = torch.nn.Linear(
#           out_features = 9,         # 10 possibilte outcomes (0, 1, 2, 3, 4, 5, 6, 7, 8)
#           in_features = 64 * 32 * 32,  # 32 images each 7 x 7 => 32 * 7 * 7 inputs
#         )

#         # self.softmax = torch.nn.Softmax(dim=1)

#     def load(self, path):
#       self.load_state_dict(torch.load(cnn_path))
#       return self.eval().to(device)

#     def forward(self, x):
#         return forward(self, x)

class MNIST_CNN(torch.nn.Module):
    def __init__(self):
        super(MNIST_CNN, self).__init__()

        ### First set of convolution ###
        self.conv_1 = torch.nn.Conv2d(
          stride = 1,          # we will convolve all of the image
          padding = 1,         # how many pixels to add in each direction
          in_channels = 3,     # we are sending in grayscale images with only 1 channel (1 image) - modify for colored ones
          kernel_size = 3,     # the size of the first set of kernels
          out_channels = 16,   # we output 16 filtered images
          # number of output images each of
          # size = (input_width − kernel_size + 2(padding_size)) / (stride) + 1
          #      = (28 - 5 + 2(2)) / 1 + 1 = 28
          # for our scenario image size: 256x256
          # size = (256 - 5 + 2*2) / 1 + 1 =  256

        )
        self.act_1 = torch.nn.ReLU() # the activation for non-linearity
        self.pool_1 = torch.nn.MaxPool2d(
          stride = 2,
          # we will convolve every other pixel splitting the size of the image by half
          # size = 28 / 2 = 14
          # size = 256 / 2 = 128
          kernel_size = 2 # size of pooling kernel
        )

        ### Second set of convolution ###
        self.conv_2 = torch.nn.Conv2d(
          stride = 1,          # we will convolve all of the image
          padding = 1,         # how many pixels to add in each direction
          kernel_size = 3,     # the size of the second set of kernels
          in_channels = 16,    # we are sending in grayscale images with 16 channels (16 images)
          out_channels = 32,   # we output 32 filtered images
          # number of output images each of
          # size = (input_width − kernel_size + 2(padding_size)) / (stride) + 1
          #      = (14 - 3 + 2(2)) / 1 + 1 = 14
        )

         # for 256x256 image
         # size = (128 - 5 + 2 *2 ) / 1 + 1 = 128

        self.act_2 = torch.nn.ReLU() # the activation for non-linearity
        self.pool_2 = torch.nn.MaxPool2d(
          stride = 2,
          # we will convolve every other pixel splitting the size of the image by half
          # size = 14 / 2 = 7
          # size = 128/2 = 64
          kernel_size = 2 # size of pooling kernel
        )

        ### third set of convolution ###
        self.conv_3 = torch.nn.Conv2d(
            stride = 1,          # we will convolve all of the image
            padding = 1,         # how many pixels to add in each direction
            kernel_size = 3,     # the size of the second set of kernels
            in_channels = 32,    # we are sending in grayscale images with 16 channels (16 images)
            out_channels = 64,   # we output 32 filtered images
        )

        self.act_3 = torch.nn.ReLU() # the activation for non-linearity
        self.pool_3 = torch.nn.MaxPool2d(
            stride = 2,
            kernel_size = 2 # size of pooling kernel
        )


        self.conv_4 = torch.nn.Conv2d(
            stride = 1,          # we will convolve all of the image
            padding = 1,         # how many pixels to add in each direction
            kernel_size = 3,     # the size of the second set of kernels
            in_channels = 64,    # we are sending in grayscale images with 16 channels (16 images)
            out_channels = 128,   # we output 32 filtered images
        )

        self.act_4 = torch.nn.ReLU() # the activation for non-linearity
        self.pool_4 = torch.nn.MaxPool2d(
            stride = 2,
            kernel_size = 2 # size of pooling kernel
        )

        self.conv_5 = torch.nn.Conv2d(
            stride = 1,          # we will convolve all of the image
            padding = 1,         # how many pixels to add in each direction
            kernel_size = 3,     # the size of the second set of kernels
            in_channels = 128,    # we are sending in grayscale images with 16 channels (16 images)
            out_channels = 256,   # we output 32 filtered images
        )

        self.act_5 = torch.nn.ReLU() # the activation for non-linearity
        self.pool_5 = torch.nn.MaxPool2d(
            stride = 2,
            kernel_size = 2 # size of pooling kernel
        )

        self.conv_6 = torch.nn.Conv2d(
            stride = 1,          # we will convolve all of the image
            padding = 1,         # how many pixels to add in each direction
            kernel_size = 3,     # the size of the second set of kernels
            in_channels = 256,    # we are sending in grayscale images with 16 channels (16 images)
            out_channels = 512,   # we output 32 filtered images
        )

        self.act_6 = torch.nn.ReLU() # the activation for non-linearity
        self.pool_6 = torch.nn.MaxPool2d(
            stride = 2,
            kernel_size = 2 # size of pooling kernel
        )
        ### Fully prediction connected layer ###
        self.out = torch.nn.Linear(
          out_features = 9,         # 10 possibilte outcomes (0, 1, 2, 3, 4, 5, 6, 7, 8)
          in_features = 512 * 4 * 4,  # 32 images each 7 x 7 => 32 * 7 * 7 inputs
        )

        # self.softmax = torch.nn.Softmax(dim=1)

    def load(self, path):
      self.load_state_dict(torch.load(cnn_path))
      return self.eval().to(device)

    def forward(self, x):
        return forward(self, x)

"""#### Back Propogation"""

'''
Trains a CNN classifier on the MNIST dataset

Inputs
  :model: CNN classifier
  :data_loader: a set of paired tuples (image, label)
'''
def train(model, data_loader, optimizer = None):

  # TODO: define the loss function to be cross-entropy loss
  loss_func = torch.nn.CrossEntropyLoss()

  if optimizer is None: optimizer = torch.optim.Adam(model.parameters(), lr = lr)
  for epoch in tqdm(range(epochs)):
    for i, (image_batch, labels) in enumerate(data_loader):
      labels = labels.to(device)
      image_batch = image_batch.to(device)
      output = model(image_batch)
      # print(f"\n[out] >> {output.shape}")
      # print(f"[labels] >> {labels.shape}")

      loss = loss_func(output, labels)

      optimizer.zero_grad()  # clear gradients for this training step
      loss.backward()        # backpropagation, compute gradients
      optimizer.step()       # apply gradients to update our kernels

    if epoch % 100 == 0:
        print (f"\nEpoch [{epoch}/{epochs}], Loss: {loss.item():.4f}")

cnn = MNIST_CNN()
cnn.to(device)
train(cnn, train_loader)

"""### Validation DataSet"""

directory_val = "/content/val_data"
final_result_val = []
convert_to_tensor = transforms.Compose([
   transforms.Resize((256, 256)),
   transforms.ToTensor(),
])


for filename in os.listdir(directory_val):
    file_path_val = os.path.join(directory_val, filename)
    name, extension = os.path.splitext(filename)
    if extension != ".txt":
      if not os.path.exists(f"{directory_val}/{name}.txt"):
        continue

      image_val = Image.open(file_path_val) #passing the img path
      image_tensor_val = convert_to_tensor(image_val)
      with open(f"{directory_val}/{name}.txt", "r") as label:
        label = label.readlines()[0].split(" ")[0]  #identify 1st character

      final_result_val.append((image_tensor_val, int(label)))
final_result_val

val_set = MyIterableDataset(final_result_val)
val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size)

for i, (image_batch, labels) in enumerate(val_loader):
  labels = labels.to(device)
  image_batch = image_batch.to(device)

  print("image batch shape:", image_batch.shape)
  print("num labels:", labels.shape)
  # display(image_batch[0].permute(1,2,0).cpu())
  predictions = cnn(image_batch)
  #print("probabilities:", predictions)

  # image_tensor = image_batch[0]
  # image_array = image_tensor.cpu().detach().numpy()
  # display(image_array[0])
  print("predictions.shape:", predictions.shape)
  # final_result_dish_index = int(predictions[0].argmax())
  predicted = []
  for i in range(len(predictions)):
    row = predictions[i]
    one_prediction = row.argmax()
    predicted.append(one_prediction.cpu())

    image = image_batch[i].permute(1,2,0).cpu()
    display(image)
    print("prediction:", int(one_prediction))
  predicted = torch.Tensor(predicted)
  print("predicted shape:", predicted.shape)
  accuracy = (predicted == labels.cpu()).sum() / len(predicted)
  print(f"accuracy: {accuracy}")
  # print("prediction probability:", prediction[0][final_result_dish_index])
  if i > 3: break

"""**სცენარები**

```
(1) kernel_size = 5, padding =2  => loss = 797
(2) kernel_size = 3, padding =1 => loss 111.2
(3) kernel_size = 7, padding =3 => loss 599.2281
```

#### Save & Load
"""

#torch.save(cnn.state_dict(), cnn_path)

#nn = MNIST_CNN().load(cnn_path)